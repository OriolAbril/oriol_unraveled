
<!DOCTYPE html>

<html data-theme="light" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="LOO-PIT tutorial" property="og:title"/>
<meta content="website" property="og:type"/>
<meta content="https://oriolabrilpla.cat/en/blog/posts/2019/loo-pit-tutorial.html" property="og:url"/>
<meta content="Oriol unraveled" property="og:site_name"/>
<meta content="Introduction: One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see..." property="og:description"/>
<meta content="https://oriolabrilpla.cat/en/_images/loo_pit_bias.png" property="og:image"/>
<meta content="" property="og:image:alt"/>
<meta content="Introduction: One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see..." name="description"/>
<title>LOO-PIT tutorial â€” Oriol unraveled</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=c5ced968eda925caa686" rel="stylesheet"/>
<link href="../../../_static/styles/bootstrap.css?digest=c5ced968eda925caa686" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=c5ced968eda925caa686" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=c5ced968eda925caa686" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css?v=a746c00c" rel="stylesheet" type="text/css"/>
<link href="../../../_static/css/style.css?v=9864b3cb" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../../_static/togglebutton.css?v=13237357" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-codeautolink.css?v=125d5c1c" rel="stylesheet" type="text/css"/>
<link href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" rel="stylesheet" type="text/css"/>
<link href="../../../_static/custom.css?v=f7eda732" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=c5ced968eda925caa686" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=c5ced968eda925caa686" rel="preload"/>
<script src="../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=c5ced968eda925caa686"></script>
<script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
<script src="../../../_static/doctools.js?v=888ff710"></script>
<script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../../_static/copybutton.js?v=f281be69"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../../../_static/design-tabs.js?v=36754332"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'blog/posts/2019/loo-pit-tutorial';</script>
<script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.2';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/OriolAbril/oriol_unraveled/sphinx/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'en';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
<link href="../../../_static/favicon.ico" rel="icon"/>
<link href="../../../about.html" rel="author" title="About these documents"/>
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<link href="https://oriolabrilpla.cat/en/blog/posts/2019/loo-pit-tutorial.html" hreflang="en" rel="alternate"/>
<link href="https://oriolabrilpla.cat/ca/blog/posts/2019/loo-pit-tutorial.html" hreflang="ca" rel="alternate"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target="#bd-toc-nav" data-default-mode="light" data-offset="180">
<a class="skip-link" href="#main">Skip to main content</a>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<header class="logo-header">
<a class="logo" href="../../../index.html">Oriol unraveled</a>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item"><nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
    Site Navigation
  </p>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects.html">
                        Projects
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../about.html">
                        About me
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../blog.html">
                        Blog
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../tag.html">
                        Tags
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../category.html">
                        Categories
                      </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links navbar-nav">
<li class="nav-item">
<a class="nav-link" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/OriolAbril" rel="noopener" target="_blank" title="GitHub"><span><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i></span>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://toot.cat/@oriolabril" rel="me" target="_blank" title="Mastodon"><span><i aria-hidden="true" class="fa-brands fa-mastodon fa-lg"></i></span>
<span class="sr-only">Mastodon</span></a>
</li>
<li class="nav-item">
<a class="nav-link" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://oriolabrilpla.cat/en//blog/atom.xml" rel="noopener" target="_blank" title="Atom Feed"><span><i aria-hidden="true" class="fa-solid fa-rss fa-lg"></i></span>
<span class="sr-only">Atom Feed</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="bd-content-nav bd-content">
<div class="bd-header-article">
<nav class="navbar navbar-expand-lg" id="navbar-main">
<label class="sidebar-toggle primary-toggle" for="__primary">
<span class="fa-solid fa-bars"></span>
</label>
<div class="article-navbar-text-links">
<nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
    Site Navigation
  </p>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects.html">
                        Projects
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../about.html">
                        About me
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../blog.html">
                        Blog
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../tag.html">
                        Tags
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../category.html">
                        Categories
                      </a>
</li>
</ul>
</nav>
</div>
<div class="article-navbar-version-switcher">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button"
      type="button"
      class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
</div>
<div class="article-navbar-search">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
</div>
<div class="article-navbar-icon-links">
<ul aria-label="Icon Links" class="navbar-icon-links navbar-nav">
<li class="nav-item">
<a class="nav-link" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/OriolAbril" rel="noopener" target="_blank" title="GitHub"><span><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i></span>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://toot.cat/@oriolabril" rel="me" target="_blank" title="Mastodon"><span><i aria-hidden="true" class="fa-brands fa-mastodon fa-lg"></i></span>
<span class="sr-only">Mastodon</span></a>
</li>
<li class="nav-item">
<a class="nav-link" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://oriolabrilpla.cat/en//blog/atom.xml" rel="noopener" target="_blank" title="Atom Feed"><span><i aria-hidden="true" class="fa-solid fa-rss fa-lg"></i></span>
<span class="sr-only">Atom Feed</span></a>
</li>
</ul>
</div>
<label class="sidebar-toggle secondary-toggle" for="__secondary">
<span class="fa-solid fa-outdent"></span>
</label>
</nav></div>
<div class="bd-content" id="main">
<div class="bd-article-container bd-content-page">
<article class="bd-article" role="main">
<section class="tex2jax_ignore mathjax_ignore" id="loo-pit-tutorial">
<span id="id1"></span><h1>LOO-PIT tutorial<a class="headerlink" href="#loo-pit-tutorial" title="Link to this heading">#</a></h1>
<img alt="" class="hidden-metadata" src="../../../_images/loo_pit_bias.png"/>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see <a class="reference external" href="https://arviz-devs.github.io/arviz/examples/plot_loo_pit_ecdf.html">two</a> <a class="reference external" href="https://arviz-devs.github.io/arviz/examples/plot_loo_pit_overlay.html">examples</a> of its usage in the example gallery and also some examples in its <a class="reference external" href="https://arviz-devs.github.io/arviz/generated/arviz.plot_loo_pit.html#arviz.plot_loo_pit">API section</a>. However, these examples are mainly related to the usage of the functionalities, not so much on the usage of LOO-PIT itself nor its interpretability.</p>
<p>I feel that the LOO-PIT algorithm usage and interpretability needs a short summary with examples showing the most common issues found when checking models with LOO-PIT. This tutorial will tackle this issue: how can LOO-PIT be used for model checking and what does it tell us in a practical manner, so we can see firsthand how wrongly specified models cause LOO-PIT values to differ from a uniform distribution. I have included a short description on what is the algorithm doing, however, for a detailed explanation, see:</p>
<ul class="simple">
<li><p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian Data Analysis. Chapman &amp; Hall/CRC Press, London, third edition. (p. 152-153)</p></li>
</ul>
<p>We will use LOO-PIT checks along with non-marginal posterior predictive checks as implemented in ArviZ. This will allow to see some differences between the two kinds of posterior predictive checks as well as to provide some intuition to cases where one may be best and cases where both are needed.</p>
<p>Here, we will experiment with LOO-PIT using two different models. First an estimation of the mean and standard deviation of a 1D Gaussian Random Variable, and then a 1D linear regression. Afterwards, we will see how to use LOO-PIT checks with multivariate data using as example a multivariate linear regression.</p>
</section>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Link to this heading">#</a></h2>
<p>One of the pilars of Bayesian Statistics is working with the posterior distribution of the parameters instead of using point estimates and errors or confidence intervals. We all know how to obtain this posterior given the likelihood, the prior and the , <span class="math notranslate nohighlight">\(p(\theta \mid y) = p(y \mid \theta) p(\theta) / p(y)\)</span>. In addition, in many cases we are also interested in <strong>the probability of future observations given the observed data</strong> according to our model. This is called posterior predictive, which is calculated integrating out <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[ p(y^* | y) = \int p(y^*|\theta) p(\theta|y) d\theta\]</div>
<p>where <span class="math notranslate nohighlight">\(y^*\)</span> is the possible unobserved data and <span class="math notranslate nohighlight">\(y\)</span> is the observed data. Therefore, if our model is correct, the observed data and the posterior predictive follow the same probability density function (pdf). In order to check if this holds, it is common to perform posterior predictive checks comparing the posterior predictive to the observed data. This can be done directly, comparing the kernel density estimates (KDE) of the observed data and posterior predictive samples, etc. A KDEs is nothing else than an estimation of the pdf of a random variable given a finite number of samples from this random variable.</p>
<p>Another alternative it to perform LOO-PIT checks, which are a kind of marginal posterior predictive checks. Marginal because we compare each observation only with the corresponding posterior predictive samples instead of combining all observations and all posterior predictive samples. As the name indicates, it combines two different concepts, Leave-One-Out Cross-Validation and Probability Integral Transform.</p>
<section id="probability-integral-transform">
<h3>Probability Integral Transform<a class="headerlink" href="#probability-integral-transform" title="Link to this heading">#</a></h3>
<p>Probability Integral Transform stands for the fact that given a random variable <span class="math notranslate nohighlight">\(X\)</span>, <strong>the random variable <span class="math notranslate nohighlight">\(Y = F_X(X) = P(x \leq X)\)</span> is a uniform random variable if the transformation <span class="math notranslate nohighlight">\(F_X\)</span> is the Cumulative Density Function</strong> (CDF) of the original random variable <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>If instead of <span class="math notranslate nohighlight">\(F_X\)</span> we have <span class="math notranslate nohighlight">\(n\)</span> samples from <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\{x_1, \dots, x_n\}\)</span>, we can use them to estimate <span class="math notranslate nohighlight">\(\hat{F_X}\)</span> and apply it to future <span class="math notranslate nohighlight">\(X\)</span> samples <span class="math notranslate nohighlight">\({x^*}\)</span>. In this case, <span class="math notranslate nohighlight">\(\hat{F_X}(x^*)\)</span> will be approximately a uniform random variable, converging to an exact uniform variable as <span class="math notranslate nohighlight">\(n\)</span> tends to infinity.</p>
<p>The mathematical demonstration can be found on wikipedia itself just googling it. However here, instead of reproducing it I will try to outline the intuition behind this fact. One way to imagine it is with posterior samples from an MCMC run. If we have enough samples, the probability of a new sample falling between the two smallest values will be the same than the probability of a new sample falling inside the two values closest to the median.</p>
<p>This is because around the probability around the smallest values will be lower, but they will be further apart, whereas the probability around the median will be larger but they will be extremely close. These two effect compensate each other and the probability is indeed the same. Thus, the probability is constant independently of the square the new sample would fall in, which is only compatible with a uniform distribution.</p>
</section>
<section id="leave-one-out-cross-validation">
<h3>Leave-One-Out Cross-Validation<a class="headerlink" href="#leave-one-out-cross-validation" title="Link to this heading">#</a></h3>
<p>Cross-Validation is one way to try to solve the problem with all the <em>future data</em> I have been mentioning so far. We do not have this future data, so how are we supposed to make calculations with it? Cross-Validation solves this problem by dividing the observed data in <span class="math notranslate nohighlight">\(K\)</span> subsets, excluding one subset from the data used to fit the model (so it is data unknown to the model, aka future data) and then using this excluded subset as future data. In general, to get better results, this process is preformed <span class="math notranslate nohighlight">\(K\)</span> times, excluding one different subset every time.</p>
<p>LOO-CV is one particular case where the number of subsets is equal to the number of observations so that each iteration only one observation is excluded. That is, we <strong>fit the model one time per observation excluding only this one observation</strong>.</p>
</section>
<section id="loo-pit">
<h3>LOO-PIT<a class="headerlink" href="#loo-pit" title="Link to this heading">#</a></h3>
<p>LOO-PIT checks consist on checking the PIT using LOO-CV. That is, fit the model on all data but observation <span class="math notranslate nohighlight">\(y_i\)</span> (we will refer to this leave one out subset as <span class="math notranslate nohighlight">\(y_{-i}\)</span>), use this model to estimate the cumulative density function of the posterior predictive and calculate the PIT, <span class="math notranslate nohighlight">\(P(y_i &lt; y^* \mid y_{-i}) = \int_{-\infty}^{y_i} p(y^* \mid y_{-i}) dy^*\)</span>, of each observation. Then, the KDE of all LOO-PIT values is estimated to see whether or not it is compatible with the LOO-PIT values being draws from a uniform variable.</p>
</section>
</section>
<section id="data-generation">
<h2>Data generation<a class="headerlink" href="#data-generation" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/index.html#module-numpy" title="numpy"><span class="nn">numpy</span></a> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span> 
<span class="kn">import</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/pyplot_summary.html#module-matplotlib.pyplot" title="matplotlib.pyplot"><span class="nn">matplotlib.pyplot</span></a> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">tt</span>

<span class="kn">import</span> <a class="sphinx-codeautolink-a" href="https://docs.scipy.org/doc/scipy/reference/stats.html#module-scipy.stats" title="scipy.stats"><span class="nn">scipy.stats</span></a> <span class="k">as</span> <span class="nn">stats</span>

<a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html#numpy.random.seed" title="numpy.random.seed"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span></a><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'arviz-darkgrid'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure" title="matplotlib.pyplot.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
    <span class="n">ax_ppc</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">);</span> <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
    <a class="sphinx-codeautolink-a" href="https://python.arviz.org/en/latest/api/generated/arviz.plot_ppc.html#arviz.plot_ppc" title="arviz.plot_ppc"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span></a><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax_ppc</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ecdf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">],</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)):</span>
        <a class="sphinx-codeautolink-a" href="https://python.arviz.org/en/latest/api/generated/arviz.plot_loo_pit.html#arviz.plot_loo_pit" title="arviz.plot_loo_pit"><span class="n">az</span><span class="o">.</span><span class="n">plot_loo_pit</span></a><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"obs"</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="n">ecdf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
    <span class="n">ax_ppc</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax_ppc</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
    <span class="k">return</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array" title="numpy.array"><span class="n">np</span><span class="o">.</span><span class="n">array</span></a><span class="p">([</span><span class="n">ax_ppc</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_obs</span> <span class="o">=</span> <span class="mi">170</span>
<span class="n">mu_normal</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="n">sd_normal</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">data_normal</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html#numpy.random.normal" title="numpy.random.normal"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span></a><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu_normal</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd_normal</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N_obs</span><span class="p">)</span>
<span class="n">a0_lr</span><span class="p">,</span> <span class="n">a1_lr</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3</span>
<span class="n">sd_lr</span> <span class="o">=</span> <span class="mf">1.4</span>
<span class="n">data_x_regression</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">N_obs</span><span class="p">)</span>
<span class="n">data_y_regression</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html#numpy.random.normal" title="numpy.random.normal"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span></a><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">a1_lr</span><span class="o">*</span><span class="n">data_x_regression</span><span class="o">+</span><span class="n">a0_lr</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd_lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coords_normal</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"obs"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"observation"</span><span class="p">],</span> <span class="s2">"log_likelihood"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"observation"</span><span class="p">]}</span>
<span class="n">dims_normal</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"observation"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_obs</span><span class="p">)}</span>
<span class="n">coords_regression</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"y"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"time"</span><span class="p">],</span> <span class="s2">"log_likelihood"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"time"</span><span class="p">]}</span>
<span class="n">dims_regression</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"time"</span><span class="p">:</span> <span class="n">data_x_regression</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We will now plot the two datsets generated, to give graphical an idea of the data we are working with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">textsize</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"axes.labelsize"</span><span class="p">]</span>
<a class="sphinx-codeautolink-a" href="https://python.arviz.org/en/latest/api/generated/arviz.plot_dist.html#arviz.plot_dist" title="arviz.plot_dist"><span class="n">az</span><span class="o">.</span><span class="n">plot_dist</span></a><span class="p">(</span><span class="n">data_normal</span><span class="p">,</span> <span class="n">rug</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rug_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">"space"</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span> <span class="n">textsize</span><span class="o">=</span><span class="n">textsize</span><span class="p">);</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data_x_regression</span><span class="p">,</span> <span class="n">data_y_regression</span><span class="p">,</span> <span class="s2">"."</span><span class="p">);</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="n">textsize</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Gaussian random variable draws"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Data for linear regression"</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/3c24c73e3dba0bd10aa3c20b78aa56e68edbea46424fe1983b76ecaa0a93e0dd.png" src="../../../_images/3c24c73e3dba0bd10aa3c20b78aa56e68edbea46424fe1983b76ecaa0a93e0dd.png"/>
</div>
</div>
</section>
<section id="unidimensional-gaussian-variable">
<h2>Unidimensional Gaussian variable<a class="headerlink" href="#unidimensional-gaussian-variable" title="Link to this heading">#</a></h2>
<p>We will start with a model that correctly fits with the data, to show how should both checks look like. Afterwards, we will see cases were these checks deviate from this ideal case and give some hints on how to interpret these deviations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Define priors</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">"sd"</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="c1"># Define likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span><span class="p">)</span>
    
    <span class="c1"># Inference!</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

    <span class="n">idata_normal</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [sd, mu]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00&lt;00:00, 4875.74draws/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01&lt;00:00, 1748.53it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal</span><span class="p">,</span> <span class="s2">"Gaussian: Calibrated model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/061046d75a9e4165e12da9c2cb64b6fd5765ce6c4bce9e37be00ecd72b1d387e.png" src="../../../_images/061046d75a9e4165e12da9c2cb64b6fd5765ce6c4bce9e37be00ecd72b1d387e.png"/>
</div>
</div>
<p>To begin with, it can be seen that <strong>the observed KDE is similar to the overlayed posterior predictive KDEs</strong>. The <strong>same happens with the LOO-PIT values</strong>; the LOO-PIT KDE is similar to the overlayed uniform KDEs. Thus, in this first example, similar information can be obteined from their interpretation.</p>
<section id="overdispersion-signs">
<h3>Overdispersion signs<a class="headerlink" href="#overdispersion-signs" title="Link to this heading">#</a></h3>
<p>We will now move to one common mismatch between the model and the observed data. We will perform the same fit as the previous example but fixing the standard deviation of the normal random variable. This is actually not an unrealistic case, as in many cases where the instrument used to measure gives error data in addition to the measure, this error is used to fix the standard deviation.</p>
<p>These two examples show how the LOO-PIT looks like for overdispersed models (i.e. the error is assumed to be larger than what it actually is) and for underdispersed models (i.e. the error is assumed to be smaller than what it really is).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">sd_normal</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span><span class="p">)</span>
    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    
    <span class="n">idata_normal_overdispersed</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00&lt;00:00, 6269.06draws/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00&lt;00:00, 3299.82it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal_overdispersed</span><span class="p">,</span> <span class="s2">"Gaussian: Overdispersed model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/15899a195f9f63de9201cd7fcd013c6189b9fa38825e4d88785d7225a8e7edf2.png" src="../../../_images/15899a195f9f63de9201cd7fcd013c6189b9fa38825e4d88785d7225a8e7edf2.png"/>
</div>
</div>
<p>In this example of <strong>overdispersed model</strong>, we can see that the posterior predictive checks show that the <strong>observed KDE is narrower than most of the posterior predictive KDEs</strong> and narrower than the mean KDE of the posterior predictive samples. However, there are still some posterior predictive samples whose KDEs are similar to the observed KDE. In the LOO-PIT check though, there is no room for confursion. <strong>The LOO-PIT KDE is not uniform between 0 and 1</strong>, its range is much quite more limited than the uniform counterparts. Moreover, the difference between the Empirical Cumulative Density Function (ECDF) and the ideal uniform CDF lays outside the envelope most of the time.</p>
</section>
<section id="underdispersion-signs">
<h3>Underdispersion signs<a class="headerlink" href="#underdispersion-signs" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">.75</span> <span class="o">*</span> <span class="n">sd_normal</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span><span class="p">)</span>
    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    
    <span class="n">idata_normal_underdispersed</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00&lt;00:00, 6293.68draws/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00&lt;00:00, 3354.84it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal_underdispersed</span><span class="p">,</span> <span class="s2">"Gaussian: Underdispersed model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/0945b57fe8063fedca5aad883a3ff892c176f2cfaab29f6c8ada4df284af8054.png" src="../../../_images/0945b57fe8063fedca5aad883a3ff892c176f2cfaab29f6c8ada4df284af8054.png"/>
</div>
</div>
<p>Here, the differences are similar to the overdispersed case, modifying overdispersed by underdispersed and inverting the shapes.</p>
</section>
<section id="bias-signs">
<h3>Bias signs<a class="headerlink" href="#bias-signs" title="Link to this heading">#</a></h3>
<p>In addition, LOO-PIT checks also show signs of model bias, as shown in the following example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">"sd"</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_normal</span> <span class="o">-</span> <span class="n">sd_normal</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span><span class="p">)</span>
    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    
    <span class="n">idata_normal_bias</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [sd]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00&lt;00:00, 4953.10draws/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01&lt;00:00, 1613.28it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal_bias</span><span class="p">,</span> <span class="s2">"Gaussian: Biased model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/f7e4e355503523a7960a9caba9e8b1170c8902946c204893112c6e78712d5420.png" src="../../../_images/f7e4e355503523a7960a9caba9e8b1170c8902946c204893112c6e78712d5420.png"/>
</div>
</div>
<p>It is important to note though, that the LOO-PIT itself already indicates the problem with the model:</p>
<ul class="simple">
<li><p>a convex KDE shape (inverted-U shape or range smaller than 0-1) or an N in the ECDF difference plot is a sign of an overdispersed model</p></li>
<li><p>a concave KDE shape (U shape) or an inverted-N ECDF difference is a sign of underdispersion</p></li>
<li><p>an asymmetrical KDE (range may also be reduced instead of 0-1) or ECDF difference is a sign for model bias</p></li>
</ul>
<p>In general though, we will probably find a combination of all these cases and it may not be straigthforward to interpretate what is wrong with the model using LOO-PIT or posterior predictive KDE checks.</p>
</section>
</section>
<section id="linear-regression">
<h2>Linear regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h2>
<p>In the case of a linear regression, the posterior predictive checks direclty do not give us much information, because each datapoint is centered at a different location, so combining them to create a single KDE wonâ€™t yield useful results.
It is important to note though, that this is not an issue inherent to the posterior predictive checks, and could be solved by rescaling each observation by substracting the mean and divide by the standard deviation along every observation from the posterior predictive. We will also include an example of this kind of transformation in the last example, but there should not be much to worry about as this improvement is on the ArviZ roadmap.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">'sigma'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">a0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"a0"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"a1"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">a0</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">*</span> <span class="n">data_x_regression</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_y_regression</span><span class="p">)</span>
    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    
    <span class="n">idata_lr</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_regression</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_regression</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [a1, a0, sigma]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:01&lt;00:00, 2882.58draws/s]
The acceptance probability does not match the target. It is 0.9065348582364419, but should be close to 0.8. Try to increase the number of tuning steps.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01&lt;00:00, 1221.16it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_lr</span><span class="p">,</span> <span class="s2">"Linear Regression: Calibrated model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/dae947d72744d55555a581d451bcf53ab26e6a53904ed54486e1f1032aa0902f.png" src="../../../_images/dae947d72744d55555a581d451bcf53ab26e6a53904ed54486e1f1032aa0902f.png"/>
</div>
</div>
<p>Now letâ€™s see how does introducing some small bias modifies the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">'sigma'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"a1"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">a0_lr</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">*</span> <span class="n">data_x_regression</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_y_regression</span><span class="p">)</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    
    <span class="n">idata_lr_bias</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_regression</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_regression</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [a1, sigma]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00&lt;00:00, 5018.12draws/s]
The acceptance probability does not match the target. It is 0.8792817423537712, but should be close to 0.8. Try to increase the number of tuning steps.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01&lt;00:00, 1413.29it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_lr_bias</span><span class="p">,</span> <span class="s2">"Linear Regression: Biased model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/51b90ffda676fcfd298af8f68792e81c9ac964e473918c02dd1ae06841f48a40.png" src="../../../_images/51b90ffda676fcfd298af8f68792e81c9ac964e473918c02dd1ae06841f48a40.png"/>
</div>
</div>
<p>Now the LOO-PIT check is clearly showing signs of bias in the model, whereas due to the lack of rescaling, no bias is seen in the posterior predictive checks.</p>
<p>Finally, letâ€™s combine some bias with overdispersion, to see how is LOO-PIT modified. Moreover, we will rescale the posterior predictive data to see how would rescaling affect the posterior predictive checks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">"a1"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="s1">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">a0_lr</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">*</span> <span class="n">data_x_regression</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">sd_lr</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_y_regression</span>
    <span class="p">)</span>
    
    <span class="c1"># Inference!</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    
    <span class="n">idata_lr_bias_overdispersed</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="n">coords_regression</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">dims_regression</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [a1]
Sampling 4 chains, 0 divergences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00&lt;00:00, 6382.96draws/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00&lt;00:00, 2138.31it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_lr_bias_overdispersed</span><span class="p">,</span> <span class="s2">"Linear Regression: Biased and oversidpersed model"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/f657b8f00a65f5d461bcde399534827164b77307edf73947028d2213565ecf33.png" src="../../../_images/f657b8f00a65f5d461bcde399534827164b77307edf73947028d2213565ecf33.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pp_samples</span> <span class="o">=</span> <span class="n">idata_lr_bias_overdispersed</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="o">.</span><span class="n">obs</span>
<span class="n">obs_samples</span> <span class="o">=</span> <span class="n">idata_lr_bias_overdispersed</span><span class="o">.</span><span class="n">observed_data</span><span class="o">.</span><span class="n">obs</span>
<span class="n">pp_means</span> <span class="o">=</span> <span class="n">pp_samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s2">"chain"</span><span class="p">,</span> <span class="s2">"draw"</span><span class="p">))</span>
<span class="n">pp_stds</span> <span class="o">=</span> <span class="n">pp_samples</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s2">"chain"</span><span class="p">,</span> <span class="s2">"draw"</span><span class="p">))</span>
<span class="n">idata_lr_bias_overdispersed</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">"obs_rescaled"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">pp_samples</span> <span class="o">-</span> <span class="n">pp_means</span><span class="p">)</span> <span class="o">/</span> <span class="n">pp_stds</span>
<span class="n">idata_lr_bias_overdispersed</span><span class="o">.</span><span class="n">observed_data</span><span class="p">[</span><span class="s2">"obs_rescaled"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs_samples</span> <span class="o">-</span> <span class="n">pp_means</span><span class="p">)</span> <span class="o">/</span> <span class="n">pp_stds</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">((</span><span class="s2">"obs"</span><span class="p">,</span> <span class="s2">"obs_rescaled"</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">ecdf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">((</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)):</span>
        <a class="sphinx-codeautolink-a" href="https://python.arviz.org/en/latest/api/generated/arviz.plot_loo_pit.html#arviz.plot_loo_pit" title="arviz.plot_loo_pit"><span class="n">az</span><span class="o">.</span><span class="n">plot_loo_pit</span></a><span class="p">(</span><span class="n">idata_lr_bias_overdispersed</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">var</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="n">ecdf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]);</span>
<a class="sphinx-codeautolink-a" href="https://python.arviz.org/en/latest/api/generated/arviz.plot_ppc.html#arviz.plot_ppc" title="arviz.plot_ppc"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span></a><span class="p">(</span><span class="n">idata_lr_bias_overdispersed</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Linear Regression: Rescaling effect</span><span class="se">\n</span><span class="s2">Biased and overdispersed model"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/606788ca81c7299469016387c38685fb24175fcad9a891f8dd15cbd38ba227bd.png" src="../../../_images/606788ca81c7299469016387c38685fb24175fcad9a891f8dd15cbd38ba227bd.png"/>
</div>
</div>
<p>As you can see, the posterior predictive check for <code class="docutils literal notranslate"><span class="pre">obs_rescaled</span></code> does indicate overdispersion and bias of the posterior predictive samples, whereas the one for <code class="docutils literal notranslate"><span class="pre">obs</span></code> does not, following what we were seeing previously. The LOO-PIT checks do not change one bit however. This is actually a property of the LOO-PIT algorithm. As it is comparing the marginal distributions of the posterior predictive and the observed data using the MCMC samples, any <em>monotonous</em> transformation will not modify its value because it wonâ€™t modify the order between the samples. Therefore, if the observed data is larger than 36% of the posterior predictive samples, the rescaling we have done does not modify this fact.</p>
<hr class="docutils"/>
<p>Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on <a class="reference external" href="https://github.com/arviz-devs/arviz/issues">ArviZ Issues</a> or <a class="reference external" href="https://discourse.pymc.io/">PyMC3 Discourse</a></p>
</section>
</section>
</article>
<footer class="bd-footer-article">
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="ablog__postcard">
<h5>
<i class="fa-regular fa-calendar-days"></i>
  31 July 2019
  
</h5>
<ul>
<li id="ablog-sidebar-item author ablog__author">
<span>
<i class="fa-solid fa-user-pen"></i>
</span>
<label class="sr-only">Author</label>
<a href="../../author/oriol-abril-pla.html">Oriol Abril Pla</a>
</li>
<li id="ablog-sidebar-item category ablog__category">
<span>
<i class="fa-regular fa-folder-open"></i>
</span>
<label class="sr-only">Category</label>
<a href="../../category/pymc.html">pymc</a>
  
  ,
  
  
  
  
  <a href="../../category/arviz.html">arviz</a>
  
  ,
  
  
  
  
  <a href="../../category/python.html">python</a>
</li>
<li id="ablog-sidebar-item tags ablog__tags">
<span>
<i class="fa-solid fa-tags"></i>
</span>
<label class="sr-only">Tags</label>
<a href="../../tag/visualization.html">visualization</a>
<a href="../../tag/model-criticism.html">model criticism</a>
</li>
</ul>
</div>
</div>
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-integral-transform">Probability Integral Transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation">Leave-One-Out Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loo-pit">LOO-PIT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-generation">Data generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unidimensional-gaussian-variable">Unidimensional Gaussian variable</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overdispersion-signs">Overdispersion signs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underdispersion-signs">Underdispersion signs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-signs">Bias signs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div id="searchbox"></div></div>
</div></div>
</div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner"></div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/bootstrap.js?digest=c5ced968eda925caa686"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=c5ced968eda925caa686"></script>
<!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//trafic-oriolabrilpla.cat/matomo/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->
<!-- <script src="../../../_static/js/intro.js"></script> -->
<div class="footer-notes">
<ul class="footer-list">
<li>Â© Oriol Abril Pla |
      <a data-toggle="tooltip" href="https://github.com/OriolAbril" rel="noopener" target="_blank" title="GitHub"><span><i class="fa-brands fa-github"></i></span><label class="sr-only">GitHub</label></a> |
      <a data-toggle="tooltip" href="https://toot.cat/@oriolabril" rel="noopener" target="_blank" title="Mastodon"><span><i class="fa-brands fa-mastodon"></i></span><label class="sr-only">Mastodon</label></a> |
      <a data-toggle="tooltip" href="https://oriolabrilpla.cat/en/blog/atom.xml" rel="noopener" target="_blank" title="Atom Feed"><span><i class="fa-solid fa-rss"></i></span><label class="sr-only">Atom Feed</label></a>
</li>
<li>Powered by <a href="https://www.sphinx-doc.org">sphinx</a></li>
<li>Design by <a href="https://pydata-sphinx-theme.readthedocs.io/">pydata-sphinx-theme</a>, <a href="https://html5up.net" rel="nofollow">HTML5 UP</a> &amp;
      <a href="https://github.com/mmistakes/jekyll-theme-basically-basic">Basically Basic</a></li>
</ul>
</div>
</body>
</html>